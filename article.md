# Учим GAN играть в шахматы


## Введение

В последние годы генеративные состязательные сети (GAN) показали впечатляющие результаты в таких областях, как фотореалистичная генерация изображений, создание абстрактных произведений искусства и сочинение музыки. Эти достижения свидетельствуют о способности искусственного интеллекта выявлять и воспроизводить сложные эстетические паттерны в визуальном и звуковом пространстве.
Но что если выйти за рамки «свободного творчества» и предложить GAN задачу, требующую соблюдения строгих логических правил? Например, генерацию объектов, где каждый элемент должен соответствовать жестким ограничениям и правилам.
Шахматная позиция является таким объектом: она строго ограничена правилами игры и требует соблюдения множества условий, от структуры фигур до стратегии игры.

На первый взгляд, задача кажется тривиальной: расставить на доске 8 × 8 несколько фигур — что тут сложного? Действительно, сгенерировать случайный набор фигур компьютер может за доли секунды. Однако разница между хаотичной расстановкой и правдоподобной позицией из реальной партии колоссальна.

Легальная шахматная позиция должна удовлетворять целому ряду ограничений:

- соблюдать правила расстановки (например, пешки не могут стоять на первой/восьмой горизонтали, у каждого игрока ровно один король);
- отражать баланс материала (преимущество в 20 пешек сразу выдаст сгенерированную позицию);
- соответствовать логике развития партии (фигуры не появляются из ниоткуда, пешки движутся только вперёд);
- учитывать тактические и стратегические паттерны (контроль центра, безопасность короля, взаимодействие фигур).

Таким образом, для генерации осмысленных позиций модель должна не просто «рисовать» фигуры на клетках, а понимать структуру шахматной игры на глубинном уровне. Это превращает задачу в нетривиальный вызов для GAN: ей предстоит освоить не только синтаксис (правила расстановки), но и семантику (смысловые закономерности позиций).

Цель этого эксперемента — проследить весь путь создания GAN для генерации шахматных позиций:

- Как подготовить данные: какие партии выбирать, как кодировать позиции, какие фильтры применять?
- Какую архитектуру сети выбрать: классические GAN, conditional GAN или более экзотические варианты?
- Как оценивать результат: по каким критериям судить о «правдоподобности» сгенерированной позиции?
- Где применить технологию: от обучения шахматистов до тестирования шахматных движков и тренировки других сетей.

Для этого разберём, как настроить генератор и дискриминатор, посмотрим на типичные ошибки и попробуем понять: способна ли GAN не просто повторять то, что видела в обучающих данных, а придумывать новые, неожиданные расстановки фигур, способные удивить даже опытных шахматистов?


## Почему шахматы — это адская задача для генеративных моделей?

На первый взгляд, шахматная доска с её строгой сеткой 8 × 8 и ограниченным набором фигур кажется идеальным объектом для алгоритмической генерации. Однако при попытке создать правдоподобную позицию генеративная модель сталкивается с каскадом нетривиальных ограничений, которые превращают задачу в настоящий интеллектуальный вызов. Разберём ключевые причины.

### Жёсткие правила легальности: нулевой допуск на ошибку

В отличие от генерации изображений или текста, где допустимы вариации и «творческая вольность», шахматные позиции подчиняются **абсолютным правилам**, не терпящим исключений. Модель обязана учитывать:
* **единственность королей**: у каждого игрока ровно один король;
* **ограничения на пешки**: пешки не могут стоять на первой/восьмой горизонтали, а также не могут появляться «из ниоткуда» (их количество строго связано с начальным составом);
* **максимальное число ферзей**: теоретически возможно до 9 ферзей у одного цвета (если все пешки дойдут до конца), но на практике такие позиции крайне редки и требуют обоснованного контекста;
* **правила хода**: например, слон всегда остаётся на клетках одного цвета, а конь перемещается буквой «Г».

Любое нарушение этих правил мгновенно делает позицию **нелегальной** — и дискриминатор (если он корректно обучен) должен это отсеивать. Для генеративной модели это означает необходимость «знать» шахматы на уровне начинающего игрока.

### Дискретность и комбинаторный взрыв: океан возможностей

Шахматная доска — это дискретное пространство с чёткими границами:
* 64 клетки;
* 13 возможных состояний для каждой клетки: 6 белых фигур, 6 чёрных фигур и пустая клетка.

Теоретически число возможных комбинаций составляет 13⁶⁴ — астрономическая величина, далеко выходящая за рамки вычислительных возможностей. На практике, конечно, подавляющее большинство этих комбинаций нелегальны или бессмысленны, но даже подмножество легальных позиций остаётся колоссальным.

Для сравнения: в генерации изображений пиксели могут плавно изменяться (например, оттенки цвета), а в шахматах замена пешки на ферзя — это **категориальное изменение**, требующее осмысленного обоснования. Модель не может «приблизить» ферзя к пешке — она должна принять чёткое решение, которое повлияет на баланс всей позиции.

### Семантическая осмысленность: от синтаксиса к смыслу

Даже если позиция легальна, этого недостаточно. Она должна выглядеть как **результат реальной игры**, а не случайный артефакт. Здесь возникают слои семантических требований:
* **баланс материала**: разница в очках (пешка = 1, конь/слон = 3, ладья = 5, ферзь = 9) не должна быть абсурдной без явных тактических обоснований;
* **типичные структуры пешек**: цепочки, изолированные пешки, двойные пешки — все они должны соответствовать логике развития;
* **взаимодействие фигур**: например, слон и конь часто дополняют друг друга, а две ладьи на одной вертикали могут создавать угрозу;
* **стратегические паттерны**: контроль центра, безопасность короля, активность фигур.

Модель должна уловить эти закономерности, которые не прописаны явно в правилах, но интуитивно понятны шахматистам. Это сродни обучению языку: сначала ты запоминаешь слова (фигуры), затем грамматику (правила хода), а потом — стилистику и контекст (стратегию).

### Контраст с генерацией изображений: категориальность vs. плавность

В классических задачах GAN (например, генерация лиц) пространство латентных переменных обычно **непрерывно**: небольшие изменения в векторе шума приводят к плавным изменениям в изображении. В шахматах же:
* каждое изменение фигуры — это **резкий скачок** в пространстве состояний;
* контекст имеет **жёсткую локальную привязку**: перемещение пешки на одну клетку может кардинально изменить оценку позиции;
* «смешивание» фигур невозможно: нельзя получить «полуферзя» или «три четверти слона».

Это создаёт проблему для генератора: ему нужно научиться делать **осмысленные дискретные выборы**, а не плавно деформировать непрерывное пространство. Традиционные методы интерполяции в латентном пространстве здесь не работают — вместо этого требуется понимание **комбинаторной логики** игры.

### Вывод

Таким образом, генерация шахматных позиций — это не просто «расстановка фигурок на доске». Это задача на стыке:
* **формальной логики** (соблюдение правил);
* **комбинаторики** (перебор допустимых вариантов);
* **семантики** (понимание смысла позиций);
* **стратегии** (предвосхищение развития игры).

Для GAN это означает необходимость выйти за рамки поверхностного копирования паттернов и научиться «думать» как шахматист — пусть и на примитивном уровне. Далее  разберем, как этого можно достичь.

## Подготовка датасетd

### Источники данных

Для обучения генеративных состязательных сетей (GAN) требуется массивный корпус шахматных партий. Поверхностное гугление дало следующие результаты:

* **ChessBase**
* **KingBase** 
* **Shredder Chess**
* **FICS Games Database**
* **ChessTempo**
* **Lichess**

Выбор пал на lichess, но при необходимости  обучающую выборку можно дополнить.

**Объём данных:** от 100 тыс. до нескольких миллионов партий — чтобы охватить разнообразие дебютов, миттельшпилей и стратегических паттернов.

### Выбор момента в партии (фильтрация по ходу)

Цель — отобрать позиции, которые:
* находятся в фазе миттельшпиля (не слишком шаблонные дебюты и не упрощённые эндшпили);
* семантически осмысленны (баланс материала, активность фигур, типичные пешечные структуры).

**Критерии отбора:**
* брать позиции с **10‑го по 40‑й ход**;
* отбрасывать:
  * дебютные повторения (ходы 1–8);
  * позиции с явным перевесом (> 10 очков по материальной оценке);
  * партии, завершившиеся до 15‑го хода (вероятно, форсированные маты или грубые ошибки).

**Обоснование:** миттельшпиль обеспечивает:
* разнообразие пешечных структур (цепи, изолированные, двойные пешки);
* взаимодействие фигур (слон + конь, ладейные рокировки);
* стратегические элементы (контроль центра, безопасность короля).


### Кодирование позиции: три способа представления доски

Задача — преобразовать шахматную позицию в числовой тензор, понятный нейросети. Рассмотрим три подхода.

**Способ 1: One‑hot encoding (8 × 8 × 13)**

* **Размерность:** 8 × 8 × 13 (64 клетки × 13 классов).
* **Классы:**
  * 0 — пустая клетка;
  * 1–6 — белые фигуры (пешка, конь, слон, ладья, ферзь, король);
  * 7–12 — чёрные фигуры (аналогично).
* **Преимущества:**
  * наглядность;
  * простота декодирования;
  * совместимость с CNN.
* **Недостатки:**
  * высокая размерность;
  * разреженность (много нулей).

**Способ 2: Знаковое кодирование (8 × 8 × 6)**
* **Размерность:** 8 × 8 × 6 (6 типов фигур).
* **Значение в канале:**
  * +1 — белая фигура;
  * -1 — чёрная фигура;
  * 0 — пусто.
* **Типы фигур (каналы):** пешка, конь, слон, ладья, ферзь, король.
* **Преимущества:**
  * компактность;
  * явное разделение по цвету.
* **Недостатки:**
  * сложнее интерпретировать;
  * требует аккуратной нормализации.

**Способ 3: Скалярное кодирование с относительными весами (8 × 8 × 1)**
* **Размерность:** 8 × 8 × 1 (одна карта признаков).
* **Значения клеток:**
  * 0 — пустая клетка;
  * 1,3,5,7,9,11 — белые фигуры;
  * 2,4,6,8
* **Преимущества:**
  * минимальная размерность;
  * учёт относительной ценности фигур;
  * удобство для регрессионных задач.
* **Недостатки:**
  * потеря детализации (нельзя различить коня и слона по значению);
  * требуется нормализация (например, деление на 10 для приведения к [-1, 1]).

**Выбор:**  
* для GAN чаще используют **one‑hot** (Способ 1) — лучше сохраняет локальную структуру;
* для регрессионных моделей — **скалярное кодирование** (Способ 3);
* **знаковое** (Способ 2) — компромисс между компактностью и детализацией.

### Нормализация и балансировка данных

**Цели:**
* уменьшить смещение в выборке;
* повысить обобщающую способность модели;
* ускорить обучение.

**Методы:**

1. **Аугментация позиций**  
   * генерировать зеркальные/повёрнутые варианты (отражение по вертикали/горизонтали, поворот на 180°);
   * увеличивает объём данных в 4 раза;
   * сохраняет семантику (баланс, пешечные структуры).

2. **Фильтрация «неинтересных» позиций**  
   * удалять:
     * позиции с материалом < 5 очков;
     * изолированные короли (эндшпильные шаблоны);
     * повторяющиеся позиции (> 3 раз в партии).
   * оставлять:
     * пешечные цепи;
     * фигуры в центре;
     * связки/вилки.

3. **Балансировка по рейтингу игроков**  
   * отбирать партии с рейтингом > 2000 (снижает шум от ошибок);
   * или стратифицировать выборку по рейтинговым диапазонам.

4. **Нормализация значений**  
   * one‑hot: 0 / 1;
   * знаковое: [-1, 1];
   * скалярное: деление на максимальный вес (например, на 10).

### Конвейер предобработки (пример на Python)

**Шаги:**
1. Скачать и распаковать PGN‑файлы (функции `download_file`, `decompress_zst_file`).
2. Прочитать партии, извлечь позиции на ходах 10–40 (`read_pgn_and_convert_to_fen`).
3. Преобразовать FEN в тензор (`fen_to_array`).
4. Применить фильтрацию и аугментацию.
5. Сохранить датасет в `.npy`

### Критерии качества датасета

* **Легальность:** все позиции валидны (проверка через `chess.Board.is_valid()`).
* **Семантика:** баланс материала, типичные структуры, отсутствие «мусора».
* **Разнообразие:** покрытие дебютов, миттельшпилей, стратегий.
* **Объём:** минимум 10⁵ позиций для стабильного обучения GAN.

**Итог:** качественно подготовленный датасет позволяет GAN генерировать не только легальные, но и **стратегически осмысленные** позиции, способные удивить даже опытных шахматистов.


## Архитектура сети 

### Базовый подход: DCGAN для шахматных досок

Идея проста: адаптировать архитектуру **Deep Convolutional GAN (DCGAN)** под дискретную структуру шахматной доски (8 × 8). Ключевое отличие от генерации изображений — выходная семантика: вместо плавных градиентов пикселей мы работаем с **категориальными значениями** (тип и цвет фигуры).

**Общий цикл обучения:**
1. Генератор получает шумовой вектор и порождает «сырую» позицию.
2. Дискриминатор оценивает, насколько позиция похожа на реальную из датасета.
3. На основе сигнала от дискриминатора генератор корректирует параметры, чтобы улучшать правдоподобность.

### Архитектура генератора

**Вход:** шумовой вектор $z$ (например, 100‑мерный).

**Этапы преобразования:**
1. Полносвязанный слой → развёртывание в тензор размерности 8 × 8 × N.
2. Серия **транспонированных свёрток** (`Conv2DTranspose`) для «наращивания» пространственной структуры.
3. Выходная активация:
* Для one‑hot кодирования (8 × 8 × 13): **Softmax** по 13 каналам для каждой клетки.
* Для скалярного кодирования (8 × 8 × 1): **Tanh** или **Sigmoid** с последующей дискретизацией.

**Пример кода (PyTorch):**
```python
class Generator(nn.Module):
    def __init__(self, latent_dim, out_channels=13):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(True),
            nn.Linear(1024, 8 * 8 * 64),
            nn.BatchNorm1d(8 * 8 * 64),
            nn.ReLU(True),
        )
        self.conv = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, out_channels, 4, 2, 1),
            nn.Softmax(dim=1)  # Softmax по каналам (13 классов)
        )

    def forward(self, z):
        x = self.model(z)
        x = x.view(x.size(0), 64, 8, 8)
        x = self.conv(x)
        return x  # Форма: (batch_size, 13, 8, 8)
```

### Архитектура дискриминатора

**Вход:** тензор позиции (8 × 8 × 13 или 8 × 8 × 6).

**Этапы обработки:**
1. Серия свёрточных слоёв (`Conv2D`) с активацией **LeakyReLU** (уклон 0.2).
2. Глобальное усреднение (`GlobalAveragePooling`) или полное развёртывание (`Flatten`).
3. Полносвязанные слои → скалярный выход (вероятность «реальности»).

**Пример кода (PyTorch):**
```python
class Discriminator(nn.Module):
    def __init__(self, in_channels=13):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
        )
        self.fc = nn.Sequential(
            nn.Linear(256 * 1 * 1, 1),  # После GlobalAveragePooling
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.model(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x  # Скаляр в [0, 1]
```

### Усложнения и модификации

1. **Conditional GAN (cGAN)**  
* **Идея:** управлять генерацией через условие (например, «баланс +2 пешки за белых»).
* **Реализация:** конкатенация условия с шумовым вектором $z$ или добавление условия в промежуточные слои.
* **Польза:** возможность генерировать позиции с заданными характеристиками.

2. **Residual блоки (ResNet‑like)**  
* Включение skip‑соединений для стабильного обучения глубоких сетей.
* Особенно полезно при увеличении размера доски (например, для шахматных вариаций).

3. **Self‑Attention слои**  
* Позволяют модели улавливать **глобальные зависимости** (например, связь между ладьёй и открытой вертикалью).
* Добавляются после свёрточных блоков.

4. **Спектральная нормализация**  
* Стабилизирует обучение, ограничивая спектральный радиус весов дискриминатора.
* Снижает риск «mode collapse» (вырождения генератора).

### Особенности обучения

1. **Функция потерь**  
* Стандартная **binary cross‑entropy** для дискриминатора и генератора.
* Альтернатива: **Wasserstein GAN (WGAN)** с градиентным штрафом для более стабильного обучения.

2. **Баланс обучения**  
* Дискриминатор должен быть достаточно сильным, чтобы давать полезный сигнал, но не «убивать» генератор.
* Практический приём: обучать дискриминатор 2–5 шагов на 1 шаг генератора.

3. **Нормализация данных**  
* Для one‑hot: значения 0 / 1.
* Для скалярного кодирования: масштабирование к [-1, 1] или [0, 1].

4. **Инициализация весов**  
* Xavier/He для свёрточных слоёв.
* Нулевая инициализация смещения (bias).

### Типичные проблемы и решения

1. **«Mode collapse»** (генератор выдаёт однообразные позиции)  
* Решение: WGAN‑GP, увеличение шума в $z$, добавление шума к входам дискриминатора.

2. **Нелегальные позиции** (два короля, лишние ферзи)  
* Решение: постобработка (коррекция через шахматный движок) или встраивание правил в архитектуру (например, через Masked Convolutions).

3. **Семантическая бессмысленность** (позицию не сыграть в реальной партии)  
* Решение: добавление «семантических» потерь (например, штраф за дисбаланс материала).

**Итог:** архитектура GAN для шахмат требует баланса между **дискретной семантикой** (правила) и **непрерывным обучением** (градиенты). Успех зависит от:
- корректного кодирования доски;
- стабильной тренировки дискриминатора;
- внедрения «шахматной логики» в метрики и архитектуру.


## Тренировочный процесс

Обучение GAN для генерации шахматных позиций — это баланс между **стабильностью** (чтобы сеть не «сходила с ума») и **выразительностью** (чтобы позиции были разнообразными и осмысленными). Разберём ключевые аспекты.

### Функция потерь

**Вариант 1: Стандартная BCE (Binary Cross‑Entropy)**  
- Используется в классических DCGAN.
- Простая реализация, но склонна к:
  - mode collapse;
  - нестабильности при сложных дискретных данных.

**Вариант 2: Wasserstein Loss (WGAN‑GP)**  
- Более плавный сигнал для генератора.
- Устойчивость к mode collapse.
- Требует:
  - градиентного штрафа (Gradient Penalty);
  - меньшего learning rate;
  - большего числа шагов дискриминатора на шаг генератора.

для шахматных позиций пробуем **WGAN‑GP** из‑за жёстких правил легальности.

### Кастомная функция потерь: разбор примера

Приведённая функция `custom_generator_loss` сочетает:
1. **WGAN‑loss** (`-torch.mean(fake_validity)`) — основной сигнал от дискриминатора.
2. **Штрафы за нарушение правил:**
* **Ровно один белый и один чёрный король** (проверка значений 11 и 12 после денормализации).
* **Не более 8 пешек каждого цвета** (проверка значений 1 и 2).
* **Пешки не могут стоять на 1‑й и 8‑й горизонтали** .

**Недостатки подхода:**
- Штрафы требуют тонкой настройки весов (`penalty_weight`).
- Не учитывает семантику (например, заблокированные фигуры).

### Практические советы по обучению

1. **Начальная стадия:**  
* Обучать дискриминатор 5–10 шагов на 1 шаг генератора.
* Использовать малый learning rate (например, 10⁻⁴).
* Контролировать loss‑кривые (резкие скачки — признак нестабильности).

2. **Стабилизация:**  
* Добавить **спектральную нормализацию** в дискриминатор.
* Применять **gradient clipping** для WGAN‑GP.
* Периодически проверять валидность позиций (например, 10 % батча).

3. **Тонкая настройка:**  
* Постепенно уменьшать вес штрафов (`penalty_weight`), когда модель начинает соблюдать правила.
* Ввести **семантический классификатор** (отдельная сеть для оценки «реалистичности» позиции).
* Экспериментировать с **температурой Softmax** на выходе генератора (влияет на разнообразие).

4. **Мониторинг:**  
* Визуализировать сгенерированные позиции каждые 100–500 шагов.
* Считать **процент легальных позиций** в батче.
* Отслеживать **дисперсию оценок Stockfish** для сгенерированных позиций.

### Метрики для оценки прогресса

1. **Процент легальных позиций** — доля валидных по правилам шахмат.  
2. **Средний материал** — баланс белых/чёрных фигур.  
3. **Уникальность** — количество неповторяющихся позиций в выборке.  
4. **Оценка движка** — средний рейтинг позиции по Stockfish (например, в санти‑пешках).  

### Чек‑лист перед запуском обучения
1. Проверить **валидность датасета** (все позиции легальны).  
2. Убедиться, что **кодирование доски** соответствует архитектуре (one‑hot vs. скалярное).  
3. Настроить **веса штрафов** в кастомной loss‑функции.  
4. Запланировать **периодическую валидацию** (например, каждые 500 шагов).  
5. Подготовить **инструменты визуализации** (SVG‑рендеринг позиций, графики loss).  
6. Определить **критерии остановки** (например % легальных позиций + стабильный loss).  

**Итог:** обучение GAN для шахмат требует:  
* комбинации стандартных техник (WGAN‑GP, спектральная нормализация);  
* кастомных штрафов за нарушение правил;  
* постоянного мониторинга семантики позиций.  

Успех зависит не столько от архитектуры, сколько от **тонкой настройки потерь и метрик**, превращающих «случайный калейдоскоп фигур» в осмысленную шахматную партию.


## Заключение

Генерация шахматных позиций с помощью GAN — нетривиальная задача на стыке **формальной логики**, **комбинаторики** и **стратегического мышления**. В ходе исследования последовательно разобрали все ключевые аспекты этой проблемы и пришли к следующим выводам:

* технология GAN действительно пригодна для генерации шахматных позиций: модель способна создавать корректные и разнообразные конфигурации фигур на доске.
* архитектуры нейросетей обладают значительным потенциалом кросс‑применения: их можно эффективно использовать не по прямому назначению, получая ощутимую пользу в смежных задачах.
* при дальнейшей доработке инструмента открывается широкое поле возможностей — в частности, для массового создания обучающих данных для других моделей (например, для шахматных движков или систем оценки позиций).
* дискриминатор GAN перспективен как инструмент мгновенной оценки шахматных позиций: его можно адаптировать для классификации позиций, поиска схожих конфигураций или выявления стратегических паттернов — однако для подтверждения эффективности требуются дополнительные тесты.

Этот процесс демонстрирует, как генеративные модели могут выходить за рамки «творческого хаоса» и осваивать **структурированные, логически жёсткие миры** — открывая путь к новым приложениям: от обучения шахматистов до тестирования шахматных движков, и поиска неочевидных стратегических паттернов.
